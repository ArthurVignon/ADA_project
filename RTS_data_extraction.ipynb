{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal is to extract data using the RTS archieve API\n",
    "\n",
    "import os\n",
    "import json\n",
    "import http.client\n",
    "import numpy as np\n",
    "\n",
    "# path to the datasets\n",
    "BASE_PATH ='../dataset'\n",
    "AUTHORIZATION = 'WWNNR3l4Wmh0UnY3bDNIc3R2QkhyTVo3eFVHWUVGYzE6MmplUjNpVHRranNNM2ZsWA=='\n",
    "RESOURCE_URL = \"https://api.srgssr.ch/rts-archives/v3/broadcasts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting for year 2019\n",
      "number of documents:31631\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_data(payload):\n",
    "    headers = {\n",
    "    'Authorization': AUTHORIZATION,\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Content-Length': '0',\n",
    "    'Postman-Token': '24264e32-2de0-f1e3-f3f8-eab014bb6d76'\n",
    "    }\n",
    "    conn = http.client.HTTPSConnection(\"api.srgssr.ch\")\n",
    "    url = \"https://api.srgssr.ch/oauth/v1/accesstoken?grant_type=client_credentials\"\n",
    "    conn.request(\"POST\",url, \"\", headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    decode_data = json.loads(data.decode(\"utf-8\"))\n",
    "    payload['Authorization'] = \"Bearer \" + decode_data['access_token']\n",
    "    \n",
    "    url = \"/rts-archives/v3/broadcasts/?minPublicationDate={}&maxPublicationDate={}&start={}\".format(payload['maxPublicationDate'],\n",
    "                                                                                                    payload['minPublicationDate'],\n",
    "                                                                                                    payload['start'])\n",
    "    conn.request(\"GET\", url, \"\", payload)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    decode_data = json.loads(data.decode(\"utf-8\"))\n",
    "    return decode_data, payload\n",
    "\n",
    "\n",
    "def build_dict(query = '', minPublicationDate = '', maxPublicationDate = '',minDurationSec = '',\n",
    "               maxDurationSec = '', mediaTypes = '', enumeratedFacets = '', publicationDateIntervalFacets = '',\n",
    "               durationSecIntervalFacets = '', start = '', rows=''):\n",
    "    \n",
    "    dict_ = {\n",
    "             'accept': \"application/json\",\n",
    "             'query' : query,\n",
    "             'minPublicationDate' : minPublicationDate,\n",
    "             'maxPublicationDate' : maxPublicationDate,\n",
    "             'minDurationSec' : minDurationSec,\n",
    "             'maxDurationSec' : maxDurationSec,\n",
    "             'mediaTypes' : mediaTypes,\n",
    "             'enumeratedFacets' : enumeratedFacets,\n",
    "             'publicationDateIntervalFacets': publicationDateIntervalFacets,\n",
    "             'durationSecIntervalFacets' : durationSecIntervalFacets,\n",
    "             'start' : '0',\n",
    "             'Postman-Token' : '56128353-805e-4974-6689-5ef6d86e2d80',\n",
    "             'rows' : rows}\n",
    "\n",
    "    return {k: v for k,v in dict_.items() if v}\n",
    "\n",
    "def request_data_and_write(payload, path=''):\n",
    "\n",
    "    data, payload = get_data(payload)\n",
    "    number_documents = data['meta']['count']\n",
    "        \n",
    "    if number_documents <= 0:\n",
    "        return\n",
    "    number_rows = 100\n",
    "    \n",
    "    print(\"number of documents:{}\".format(number_documents))\n",
    "    \n",
    "    if 'rows' in payload and payload['rows'] > 0:\n",
    "        number_rows = payload['rows']\n",
    "        \n",
    "    number_requests = (number_documents + number_rows // 2) // number_rows\n",
    "    \n",
    "    if not os.path.exists(BASE_PATH + \"hello\"):\n",
    "        os.makedirs(BASE_PATH + path)\n",
    "\n",
    "    with open(BASE_PATH + path + 'data_0.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "    for n in range(1,number_requests):\n",
    "        \n",
    "        #Update payload\n",
    "        payload['start'] = n * number_rows\n",
    "\n",
    "        data = get_data(payload)\n",
    "        \n",
    "        with open(BASE_PATH + path + 'data_{}.json'.format(n), 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "def request_data_per_year(year):\n",
    "    print('requesting for year ' + str(year))\n",
    "    payload = build_dict(minPublicationDate='{}'.format(year), maxPublicationDate='{}'.format(year))\n",
    "    request_data_and_write(payload, '_per_year/{}/'.format(year))\n",
    "    \n",
    "\n",
    "years = np.arange(2010, 2020)   \n",
    "for year in years:\n",
    "    request_data_per_year(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
